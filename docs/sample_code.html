<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Sample Code and Tutorials &mdash; AI Vision Library Documentation 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=0718c456" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=2709fde1"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Best Practices" href="best_practices.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #003366" >

          
          
          <a href="index.html" class="icon icon-home">
            AI Vision Library Documentation
              <img src="_static/images.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="detect_objects.html">Detect Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="recognize_objects.html">Recognize Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="train_model.html">Train Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="error_handling.html">Error Handling and Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="best_practices.html">Best Practices</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Sample Code and Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#example-1-real-time-object-detection-with-live-visualization"><strong>Example 1: Real-Time Object Detection with Live Visualization</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="#example-2-training-a-custom-object-recognition-model-with-data-augmentation"><strong>Example 2: Training a Custom Object Recognition Model with Data Augmentation</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="#example-3-integrating-with-ros-for-autonomous-drone-navigation"><strong>Example 3: Integrating with ROS for Autonomous Drone Navigation</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="#example-4-video-analytics-with-multi-object-tracking"><strong>Example 4: Video Analytics with Multi-Object Tracking</strong></a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #003366" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AI Vision Library Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Sample Code and Tutorials</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/sample_code.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="sample-code-and-tutorials">
<h1>Sample Code and Tutorials<a class="headerlink" href="#sample-code-and-tutorials" title="Link to this heading"></a></h1>
<p>Welcome to the Sample Code and Tutorials section of the AI Vision Library documentation! This section is packed with practical examples, advanced tutorials, and best practices to help you master the library. Whether you’re integrating AI vision into cutting-edge robotics or experimenting with new computer vision models, these examples will guide you every step of the way.</p>
<section id="example-1-real-time-object-detection-with-live-visualization">
<h2><strong>Example 1: Real-Time Object Detection with Live Visualization</strong><a class="headerlink" href="#example-1-real-time-object-detection-with-live-visualization" title="Link to this heading"></a></h2>
<p>In this example, we’ll walk through how to use the AI Vision Library to perform real-time object detection with live visualization. We’ll leverage the power of GPU acceleration to detect objects at lightning speed and display the results in real-time.</p>
<p><strong>Code Example</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ai_vision_library</span> <span class="kn">import</span> <span class="n">detect_objects</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="c1"># Initialize video capture from the webcam</span>
<span class="n">video_capture</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Use 0 for the default camera</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="c1"># Capture frame-by-frame</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">video_capture</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Failed to capture image&quot;</span><span class="p">)</span>
        <span class="k">break</span>

    <span class="c1"># Perform object detection on the frame</span>
    <span class="n">detected_objects</span> <span class="o">=</span> <span class="n">detect_objects</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

    <span class="c1"># Draw bounding boxes and labels on the frame</span>
    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">detected_objects</span><span class="p">:</span>
        <span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">obj</span><span class="p">[</span><span class="s1">&#39;bbox&#39;</span><span class="p">]</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="p">),</span> <span class="p">(</span><span class="n">x_max</span><span class="p">,</span> <span class="n">y_max</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">putText</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">obj</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">obj</span><span class="p">[</span><span class="s1">&#39;confidence&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="o">-</span><span class="mi">10</span><span class="p">),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FONT_HERSHEY_SIMPLEX</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Display the resulting frame</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;Real-Time Object Detection&#39;</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>

    <span class="c1"># Break the loop on &#39;q&#39; key press</span>
    <span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;q&#39;</span><span class="p">):</span>
        <span class="k">break</span>

<span class="c1"># Release the capture and close any OpenCV windows</span>
<span class="n">video_capture</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Expected Output</strong>:</p>
<p>Running this code will open a window displaying the live camera feed with bounding boxes and labels around detected objects. The real-time performance, powered by CUDA, should allow for seamless interaction.</p>
</section>
<section id="example-2-training-a-custom-object-recognition-model-with-data-augmentation">
<h2><strong>Example 2: Training a Custom Object Recognition Model with Data Augmentation</strong><a class="headerlink" href="#example-2-training-a-custom-object-recognition-model-with-data-augmentation" title="Link to this heading"></a></h2>
<p>This example covers how to train a custom object recognition model using your dataset, enhanced with data augmentation to improve generalization. We’ll load the dataset, apply augmentation, configure training parameters, and save the trained model.</p>
<p><strong>Code Example</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ai_vision_library</span> <span class="kn">import</span> <span class="n">train_model</span>
<span class="kn">from</span> <span class="nn">ai_vision_library.datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">augment_data</span>

<span class="c1"># Load and augment your dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;path_to_dataset&#39;</span><span class="p">)</span>
<span class="n">augmented_dataset</span> <span class="o">=</span> <span class="n">augment_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">flip</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rotate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Configure training parameters</span>
<span class="n">architecture</span> <span class="o">=</span> <span class="s1">&#39;resnet&#39;</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span>

<span class="c1"># Train the model with data augmentation</span>
<span class="n">model</span><span class="p">,</span> <span class="n">logs</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">augmented_dataset</span><span class="p">,</span> <span class="n">architecture</span><span class="o">=</span><span class="n">architecture</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Save the trained model</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;my_augmented_model.h5&#39;</span><span class="p">)</span>

<span class="c1"># Output training logs</span>
<span class="k">for</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">metrics</span> <span class="ow">in</span> <span class="n">logs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">: Loss = </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Accuracy = </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Advanced Notes</strong>:</p>
<ul class="simple">
<li><p><strong>Data Augmentation</strong>: Applying transformations like flipping, rotating, and scaling to your dataset helps create a more robust model that generalizes better to unseen data.</p></li>
<li><p><strong>Training Performance</strong>: By increasing the batch size and using a powerful architecture like ResNet, we can leverage GPU acceleration to train complex models more efficiently.</p></li>
</ul>
</section>
<section id="example-3-integrating-with-ros-for-autonomous-drone-navigation">
<h2><strong>Example 3: Integrating with ROS for Autonomous Drone Navigation</strong><a class="headerlink" href="#example-3-integrating-with-ros-for-autonomous-drone-navigation" title="Link to this heading"></a></h2>
<p>In this advanced tutorial, we’ll integrate the AI Vision Library with ROS to perform real-time object detection for autonomous drone navigation. The detected objects will be used to adjust the drone’s flight path dynamically.</p>
<p><strong>Code Example</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">rospy</span>
<span class="kn">from</span> <span class="nn">sensor_msgs.msg</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">ai_vision_library</span> <span class="kn">import</span> <span class="n">detect_objects</span>
<span class="kn">from</span> <span class="nn">geometry_msgs.msg</span> <span class="kn">import</span> <span class="n">Twist</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">cv_bridge</span> <span class="kn">import</span> <span class="n">CvBridge</span>

<span class="c1"># Initialize ROS node</span>
<span class="n">rospy</span><span class="o">.</span><span class="n">init_node</span><span class="p">(</span><span class="s1">&#39;drone_vision_node&#39;</span><span class="p">)</span>

<span class="c1"># Initialize publishers and subscribers</span>
<span class="n">vel_pub</span> <span class="o">=</span> <span class="n">rospy</span><span class="o">.</span><span class="n">Publisher</span><span class="p">(</span><span class="s1">&#39;/cmd_vel&#39;</span><span class="p">,</span> <span class="n">Twist</span><span class="p">,</span> <span class="n">queue_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">bridge</span> <span class="o">=</span> <span class="n">CvBridge</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">image_callback</span><span class="p">(</span><span class="n">msg</span><span class="p">):</span>
    <span class="c1"># Convert the ROS image message to OpenCV format</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">bridge</span><span class="o">.</span><span class="n">imgmsg_to_cv2</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="s2">&quot;bgr8&quot;</span><span class="p">)</span>

    <span class="c1"># Perform object detection</span>
    <span class="n">detected_objects</span> <span class="o">=</span> <span class="n">detect_objects</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

    <span class="c1"># Example logic: Adjust flight based on detected objects</span>
    <span class="n">twist</span> <span class="o">=</span> <span class="n">Twist</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">detected_objects</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">obj</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;obstacle&#39;</span><span class="p">):</span>
            <span class="n">twist</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span>  <span class="c1"># Move backward</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">twist</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Move forward</span>

        <span class="n">vel_pub</span><span class="o">.</span><span class="n">publish</span><span class="p">(</span><span class="n">twist</span><span class="p">)</span>
        <span class="n">rospy</span><span class="o">.</span><span class="n">loginfo</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Detected </span><span class="si">{</span><span class="n">obj</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="n">obj</span><span class="p">[</span><span class="s1">&#39;confidence&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% confidence&quot;</span><span class="p">)</span>

<span class="c1"># Subscribe to the drone&#39;s camera feed</span>
<span class="n">image_sub</span> <span class="o">=</span> <span class="n">rospy</span><span class="o">.</span><span class="n">Subscriber</span><span class="p">(</span><span class="s1">&#39;/drone/camera/image_raw&#39;</span><span class="p">,</span> <span class="n">Image</span><span class="p">,</span> <span class="n">image_callback</span><span class="p">)</span>

<span class="c1"># Keep the node running</span>
<span class="n">rospy</span><span class="o">.</span><span class="n">spin</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Expected Output</strong>:</p>
<p>This script processes the drone’s live camera feed in real-time, detecting obstacles and dynamically adjusting the drone’s velocity to avoid collisions.</p>
</section>
<section id="example-4-video-analytics-with-multi-object-tracking">
<h2><strong>Example 4: Video Analytics with Multi-Object Tracking</strong><a class="headerlink" href="#example-4-video-analytics-with-multi-object-tracking" title="Link to this heading"></a></h2>
<p>Here, we’ll demonstrate how to process a video file for multi-object tracking, performing detection on each frame and tracking objects across the video.</p>
<p><strong>Code Example</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ai_vision_library</span> <span class="kn">import</span> <span class="n">detect_objects</span><span class="p">,</span> <span class="n">track_objects</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="c1"># Load a video file</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="s1">&#39;sample_video.mp4&#39;</span><span class="p">)</span>

<span class="c1"># Initialize multi-object tracker</span>
<span class="n">tracker</span> <span class="o">=</span> <span class="n">track_objects</span><span class="p">()</span>

<span class="k">while</span> <span class="n">video</span><span class="o">.</span><span class="n">isOpened</span><span class="p">():</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">video</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># Perform object detection on the frame</span>
    <span class="n">detected_objects</span> <span class="o">=</span> <span class="n">detect_objects</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

    <span class="c1"># Update tracker with detected objects</span>
    <span class="n">tracked_objects</span> <span class="o">=</span> <span class="n">tracker</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">detected_objects</span><span class="p">)</span>

    <span class="c1"># Draw bounding boxes and labels on the frame</span>
    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">tracked_objects</span><span class="p">:</span>
        <span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">obj</span><span class="p">[</span><span class="s1">&#39;bbox&#39;</span><span class="p">]</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="p">),</span> <span class="p">(</span><span class="n">x_max</span><span class="p">,</span> <span class="n">y_max</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">putText</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">obj</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> ID:</span><span class="si">{</span><span class="n">obj</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">obj</span><span class="p">[</span><span class="s1">&#39;confidence&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="o">-</span><span class="mi">10</span><span class="p">),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FONT_HERSHEY_SIMPLEX</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Display the resulting frame</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;Video Analytics - Multi-Object Tracking&#39;</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>

    <span class="c1"># Save the processed frame to the output video</span>
    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>

    <span class="c1"># Break the loop on &#39;q&#39; key press</span>
    <span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;q&#39;</span><span class="p">):</span>
        <span class="k">break</span>

<span class="c1"># Release everything when the job is finished</span>
<span class="n">video</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Advanced Notes</strong>:</p>
<ul class="simple">
<li><p><strong>Multi-Object Tracking</strong>: This example introduces object tracking, allowing the AI Vision Library to not only detect but also track objects across video frames, maintaining consistent IDs.</p></li>
<li><p><strong>Video Processing</strong>: Ideal for security applications, autonomous vehicles, and advanced robotics, this example demonstrates how to handle video data efficiently.</p></li>
</ul>
<p><strong>Performance Tips</strong>:</p>
<ul class="simple">
<li><p><strong>Leverage GPU Acceleration</strong>: Always use CUDA or other GPU acceleration options available to you for real-time performance, especially in high-throughput scenarios like video analytics.</p></li>
<li><p><strong>Optimize Thresholds</strong>: Fine-tune detection thresholds based on your specific application requirements to balance accuracy and processing speed.</p></li>
<li><p><strong>Batch Processing</strong>: For large-scale data processing, batch operations can significantly reduce overhead and improve efficiency.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="best_practices.html" class="btn btn-neutral float-left" title="Best Practices" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Griffin Thompson.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>